{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICM Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Music Style Recombination and Interpolation in General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # use the print() function from Python3\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa             # The librosa library\n",
    "import librosa.display\n",
    "from pylab import *\n",
    "import IPython.display     # IPython's display module (for in-line audio)\n",
    "import madmom              # madmom MIR library\n",
    "import spleeter            # spleeter source separation\n",
    "from spleeter.separator import Separator\n",
    "import numpy as np              # numpy  numerical functions\n",
    "import ffmpeg\n",
    "SR = 44100                 # default sample rate\n",
    "HOP_LENGTH = 1024           # default hop length\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import pretty_midi as pm\n",
    "import os\n",
    "from operator import add, itemgetter\n",
    "import math\n",
    "import random\n",
    "\n",
    "if not os.path.exists('./demo'):\n",
    "    os.mkdir('./demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Use MIR to extract fundamental frequency, chords, beat and pitch onset from input mp3 files, generate corresponding melody midi and chord midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spleeter_separate(file_name, mode=\"spleeter:2stems\", sr=44100):\n",
    "    mix, _ = librosa.load(file_name, sr=sr)\n",
    "    separator = Separator(mode)\n",
    "    predicted_sources = separator.separate(mix[:, None])\n",
    "    if not os.path.exists(f'{file_name[:-4]}_process_audio'):\n",
    "        os.mkdir(f'{file_name[:-4]}_processed_audio')\n",
    "    for key in predicted_sources:\n",
    "        separated_source = predicted_sources[key][:, 0] # remove the channel dimension\n",
    "        sf.write(f'{file_name[:-4]}_processed_audio/{file_name[:-4]}_{key}.wav', separated_source, sr)\n",
    "    print('Vocal successfully extracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pitch_extraction_advanced(file_name, time_threshold=0.2, pitch_threshold=0.5, sample_rate=44100, frame_length=2048):\n",
    "    # Implement pYIN to obtain f0 in Hz\n",
    "    wave, _ = librosa.load(file_name, sample_rate)\n",
    "    hop_length = frame_length//4\n",
    "    f0, voiced_flag, voiced_probs = librosa.pyin(wave, \n",
    "                                                 sr=sample_rate, \n",
    "                                                 frame_length=frame_length,\n",
    "                                                 hop_length= hop_length, \n",
    "                                                 fmin=librosa.note_to_hz('C2'), \n",
    "                                                 fmax=librosa.note_to_hz('C7'))\n",
    "    times = librosa.times_like(f0, sr=sample_rate, hop_length= hop_length)\n",
    "    \n",
    "    f = open(f'{file_name[:-4]}_f0.txt', 'w')\n",
    "    for time, freq in zip(times, f0):\n",
    "        if (not np.isnan(freq)):\n",
    "            f.write('%f\\t%f\\n' % (time, freq))\n",
    "    f.close()\n",
    "    \n",
    "    # Separate f0 and times by NaN (array of array)\n",
    "    f0_nested = np.array([f0[s] for s in np.ma.clump_unmasked(np.ma.masked_invalid(f0))])\n",
    "    times_nested = np.array([times[s] for s in np.ma.clump_unmasked(np.ma.masked_invalid(f0))])\n",
    "    \n",
    "    # Delete the lists that have too little instances (list of array)\n",
    "    for i in range(len(f0_nested)):\n",
    "        if times_nested[i][-1] - times_nested[i][0] < time_threshold:\n",
    "            f0_nested_filtered = np.delete(f0_nested, i, axis=0)\n",
    "            times_nested_filtered = np.delete(times_nested, i, axis=0)\n",
    "    try:\n",
    "        f0_nested_filtered\n",
    "        times_nested_filtered\n",
    "    except:\n",
    "        f0_nested_filtered = f0_nested\n",
    "        times_nested_filtered = times_nested\n",
    "        \n",
    "    f0_nested_filtered = list(f0_nested_filtered)\n",
    "    times_nested_filtered = list(times_nested_filtered)\n",
    "\n",
    "    \n",
    "    # Wash the lists based on thresholds\n",
    "    pitch_index_nested = []\n",
    "    time_nested_new = []\n",
    "    for list_idx in range(len(f0_nested_filtered)):\n",
    "        pitch_index_list = []\n",
    "        time_list_new = []\n",
    "        for i in range(len(f0_nested_filtered[list_idx])):\n",
    "            if i == 0:\n",
    "                pitch_index_list.append(i)\n",
    "                time_list_new.append(times_nested_filtered[list_idx][i])\n",
    "                \n",
    "            else:\n",
    "                current = librosa.hz_to_midi(f0_nested_filtered[list_idx][i])\n",
    "                previous = librosa.hz_to_midi(f0_nested_filtered[list_idx][i-1])\n",
    "                if abs(current-previous) > pitch_threshold and times_nested_filtered[list_idx][i] - time_list_new[-1] > time_threshold:\n",
    "                    pitch_index_list.append(i)\n",
    "                    time_list_new.append(times_nested_filtered[list_idx][i])\n",
    "                    \n",
    "        time_list_new.append(times_nested_filtered[list_idx][-1])\n",
    "        pitch_index_list = np.array(pitch_index_list)\n",
    "        pitch_index_nested.append(pitch_index_list)\n",
    "        \n",
    "        time_list_new = np.array(time_list_new)\n",
    "        time_nested_new.append(time_list_new)\n",
    "            \n",
    "    # Convert the index to pitch (by taking average)\n",
    "    pitch_list = []\n",
    "    for list_idx in range(len(pitch_index_nested)):\n",
    "        pitch = []\n",
    "        for i in range(len(pitch_index_nested[list_idx])):\n",
    "            try:\n",
    "                freq_group = f0_nested_filtered[list_idx][pitch_index_nested[list_idx][i]:pitch_index_nested[list_idx][i+1]]\n",
    "            except:\n",
    "                freq_group = f0_nested_filtered[list_idx][pitch_index_nested[list_idx][i]:-1]\n",
    "            \n",
    "            if list(freq_group) == []:\n",
    "                freq_group = [f0_nested_filtered[list_idx][pitch_index_nested[list_idx][i]]]\n",
    "            freq_avg = np.mean(freq_group)\n",
    "            pitch_avg = round(librosa.hz_to_midi(freq_avg))\n",
    "            pitch.append(pitch_avg)\n",
    "        pitch = np.array(pitch)\n",
    "        pitch_list.append(pitch)\n",
    "\n",
    "    # Generate\n",
    "    music = pm.PrettyMIDI(initial_tempo=75)\n",
    "    piano = pm.Instrument(program=1)\n",
    "    for pitch_array, time_array in zip(pitch_list, time_nested_new):\n",
    "        for i in range(len(pitch_array)):\n",
    "            pitch = pitch_array[i]\n",
    "            start = time_array[i]\n",
    "            end = time_array[i+1]\n",
    "            if start == end:\n",
    "                end = start + max(time_threshold/5, 0.2)\n",
    "                \n",
    "            note = pm.Note(velocity=100, pitch=pitch, start=start, end=end)\n",
    "            piano.notes.append(note)\n",
    "    music.instruments.append(piano)\n",
    "    music.write(f'{file_name[:-4]}_extracted_pitch.mid')\n",
    "    print('Pitch extraction successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from madmom.features import DBNDownBeatTrackingProcessor, RNNDownBeatProcessor, CNNChordFeatureProcessor, CRFChordRecognitionProcessor, PeakPickingProcessor, NotePeakPickingProcessor, RNNPianoNoteProcessor\n",
    "from madmom.processors import ParallelProcessor, Processor, SequentialProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract chords from the audio\n",
    "in_processor = CNNChordFeatureProcessor()\n",
    "chord_processor = CRFChordRecognitionProcessor()\n",
    "\n",
    "def get_chord(fn):\n",
    "    data = in_processor.process(fn)\n",
    "    chords = chord_processor.process(data)\n",
    "    f = open(f'{fn[:-4]}_processed_audio/{fn[:-4]}_chords.txt', 'w')\n",
    "    for line in chords:\n",
    "        f.write('%f\\t%f\\t%s\\n' % (line[0], line[1], line[2]))\n",
    "    f.close()\n",
    "    return chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melody_and_chords_extraction(fn):\n",
    "    spleeter_separate(fn)\n",
    "    pitch_extraction_advanced(f'{fn[:-4]}_processed_audio/{fn[:-4]}_vocals.wav', time_threshold=0.2, pitch_threshold=0.5, sample_rate=44100, frame_length=2048)\n",
    "    chords = get_chord(fn)\n",
    "    melody_name = f'{fn[:-4]}_processed_audio/{fn[:-4]}_vocals_extracted_pitch.mid'\n",
    "    \n",
    "    return melody_name, chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Quantize generated melody midi and chord midi and transform them into list of two-measure notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ec2vae.model import EC2VAE\n",
    "# initialize the model\n",
    "ec2vae_model = EC2VAE.init_model()\n",
    "\n",
    "# load model parameter\n",
    "ec2vae_param_path = './ec2vae/model_param/ec2vae-v1.pt'\n",
    "ec2vae_model.load_model(ec2vae_param_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the time in seconds, output the subbeat index of range 0 .. len(keypoints) - 1\n",
    "def quantize(keypoint_boundaries, time):\n",
    "    return np.searchsorted(keypoint_boundaries, time)\n",
    "\n",
    "def get_quantized_notes_and_chords(name_of_melody_file, chords):\n",
    "    #get beat\n",
    "    print(len(chords))\n",
    "    midi_data = pm.PrettyMIDI(name_of_melody_file)\n",
    "    display(midi_data.instruments)\n",
    "    beats = midi_data.get_beats()\n",
    "    print(beats[:10])\n",
    "    beat_id = np.arange(len(beats))\n",
    "    subbeat_id = np.arange(len(beats) * 4) / 4.0\n",
    "    keypoints = np.interp(subbeat_id, beat_id, beats)\n",
    "    keypoint_boundaries = (keypoints[1:] + keypoints[:-1]) / 2\n",
    "    #melody quantization\n",
    "    quantized_notes = [{\n",
    "    'start_quantized': quantize(keypoint_boundaries, note.start),\n",
    "    'end_quantized': quantize(keypoint_boundaries, note.end),\n",
    "    'pitch': note.pitch,\n",
    "    'velocity' : note.velocity,\n",
    "    } for note in midi_data.instruments[0].notes]\n",
    "    \n",
    "    #chord quantization\n",
    "    quantized_chords = [{\n",
    "    'start_quantized': quantize(keypoint_boundaries, float(chord[0])),\n",
    "    'end_quantized': quantize(keypoint_boundaries, float(chord[1])),\n",
    "    'chord': chord[2],\n",
    "    'velocity' : 100,\n",
    "    } for chord in chords]\n",
    "\n",
    "    return quantized_notes, quantized_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample chords.\n",
    "chord_dic = {}\n",
    "chord_dic['A:maj'] = [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
    "chord_dic['A:min'] = [1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
    "chord_dic['A#:maj'] = [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
    "chord_dic['A#:min'] = [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
    "chord_dic['B:maj'] = [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
    "chord_dic['B:min'] = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
    "chord_dic['C:maj'] = [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0]\n",
    "chord_dic['C:min'] = [1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "chord_dic['C#:maj'] = [0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "chord_dic['C#:min'] = [0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
    "chord_dic['D:maj'] = [0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
    "chord_dic['D:min'] = [0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "chord_dic['D#:maj'] = [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0]\n",
    "chord_dic['D#:min'] = [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "chord_dic['E:maj'] = [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
    "chord_dic['E:min'] = [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]\n",
    "chord_dic['F:maj'] = [1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n",
    "chord_dic['F:min'] = [1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "chord_dic['F#:maj'] = [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "chord_dic['F#:min'] = [0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]\n",
    "chord_dic['G:maj'] = [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n",
    "chord_dic['G:min'] = [0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0]\n",
    "chord_dic['G#:maj'] = [1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0]\n",
    "chord_dic['G#:min'] = [0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
    "chord_dic['N'] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_measures_list(quantized_notes, quantized_chords):\n",
    "    #Generate List of two measures notes\n",
    "    print(len(quantized_notes), len(quantized_chords))\n",
    "    score = []\n",
    "    last_end = 0\n",
    "    for note in quantized_notes:\n",
    "        if note['start_quantized'] > last_end:\n",
    "            for i in range(note['start_quantized']-last_end):\n",
    "                score.append(129)\n",
    "        duration = note['end_quantized'] - note['start_quantized']\n",
    "        for i in range(duration):\n",
    "            if i == 0:\n",
    "                score.append(note['pitch'])\n",
    "            else:\n",
    "                score.append(128)\n",
    "            \n",
    "        last_end = note['end_quantized']\n",
    "    \n",
    "    l_two_measures = []\n",
    "    n = 0\n",
    "    while 32*(n+1) < len(score):\n",
    "        l_two_measures.append(np.array(score[32*n:32*(n+1)]))\n",
    "        n += 1  \n",
    "    last_two_measure = score[32*n:]\n",
    "    while len(last_two_measure) < 32:\n",
    "        last_two_measure.append(129)\n",
    "    \n",
    "    l_two_measures.append(np.array(last_two_measure))\n",
    "    \n",
    "    #Generate List of two measures chords\n",
    "    chord_score = []\n",
    "    last_end = quantized_chords[0]['start_quantized']\n",
    "    for chord in quantized_chords:\n",
    "        if chord['start_quantized'] > last_end:\n",
    "            for i in range(chord['start_quantized']-last_end):\n",
    "                chord_score.append(chord_dic['N'])\n",
    "        duration = chord['end_quantized'] - chord['start_quantized']\n",
    "        for i in range(duration):\n",
    "            chord_score.append(chord_dic[chord['chord']])\n",
    "            \n",
    "        last_end = chord['end_quantized']\n",
    "    \n",
    "    l_two_measure_chords = []\n",
    "    n = 0\n",
    "    while 32*(n+1) < len(chord_score):\n",
    "        l_two_measure_chords.append(np.array(chord_score[32*n:32*(n+1)]))\n",
    "        n += 1\n",
    "    \n",
    "    last_two_measure_chords = chord_score[32*n:]\n",
    "    while len(last_two_measure_chords) < 32:\n",
    "        last_two_measure_chords.append(chord_dic['N'])\n",
    "    \n",
    "    l_two_measure_chords.append(np.array(last_two_measure_chords))\n",
    "    \n",
    "    while len(l_two_measure_chords) < len(l_two_measures):\n",
    "        l_two_measure_chords.append(np.array([chord_dic['N']]*32))\n",
    "        \n",
    "    \n",
    "    return l_two_measures, l_two_measure_chords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn note arrays into one-hot vectors\n",
    "def note_array_to_onehot(note_array):\n",
    "    pr = np.zeros((len(note_array), 130))\n",
    "    pr[np.arange(0, len(note_array)), note_array.astype(int)] = 1.\n",
    "    return pr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Style Recombination and Interpolation, Generate Raw Piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_list(l_two_measures, l_two_measure_chords):\n",
    "    pr_list = [note_array_to_onehot(x) for x in l_two_measures]\n",
    "    #Convert to pytorch tensors, and to cuda/cpu\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Concert\n",
    "    pr_list = [torch.from_numpy(pr).float().to(device).unsqueeze(0) for pr in pr_list]\n",
    "    #chord list\n",
    "    l_c = [torch.from_numpy(c).float().to(device).unsqueeze(0) for c in l_two_measure_chords]\n",
    "    # encode list of two-measure melodies and chords\n",
    "    z_list = []\n",
    "    for i in range(len(pr_list)):\n",
    "        zp, zr = ec2vae_model.encoder(pr_list[i], l_c[i])\n",
    "        z_list.append([zp, zr])\n",
    "        \n",
    "    return z_list, l_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_midi_with_melody_chord(fn, mel_notes, c_notes, tempo):\n",
    "    midi = pm.PrettyMIDI(initial_tempo = tempo)\n",
    "    ins1 = pm.Instrument(0)\n",
    "    ins1.notes = mel_notes\n",
    "    ins2 = pm.Instrument(0)\n",
    "    ins2.notes = c_notes\n",
    "    midi.instruments.append(ins1)\n",
    "    midi.instruments.append(ins2)\n",
    "    midi.write(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_raw_piece(z_list_pitch, z_list_rhythm, l_c, beats, fn):\n",
    "    # Use pitch of the piece and rhythm of another piece\n",
    "    l_new_rhythm = []\n",
    "    for i in range(len(z_list_pitch)):\n",
    "        if i > len(z_list_rhythm) - 1:\n",
    "            pred_piece = ec2vae_model.decoder(z_list_pitch[i][0], z_list_rhythm[i%len(z_list_rhythm)][1], l_c[i])\n",
    "        else:\n",
    "            pred_piece = ec2vae_model.decoder(z_list_pitch[i][0], z_list_rhythm[i][1], l_c[i])\n",
    "        l_new_rhythm.append(pred_piece)\n",
    "    \n",
    "    out_list = []\n",
    "    for i in l_new_rhythm:\n",
    "        out_new = i.squeeze(0).cpu().numpy()\n",
    "        out_list.append(out_new)\n",
    "        \n",
    "    notes_list = []\n",
    "    for i in range(len(out_list)):\n",
    "        notes = ec2vae_model.__class__.note_array_to_notes(out_list[i], bpm=60/beats, start=8*beats*i)\n",
    "        notes_list.extend(notes)\n",
    "        \n",
    "    all_notes_l_c = []   \n",
    "    #notes_l_c = [ec2vae_model.__class__.chord_to_notes(c.squeeze(0).cpu().numpy(), 60/beats, 0) for c in l_c]\n",
    "    notes_l_c = [ec2vae_model.__class__.chord_to_notes(l_c[a].squeeze(0).cpu().numpy(), 60/beats, 8*beats*a) for a in range(len(l_c))]\n",
    "    for i in notes_l_c:\n",
    "        all_notes_l_c.extend(i)\n",
    "    \n",
    "    #for i in range(len(out_list)):\n",
    "        #for note in notes_l_c[i]:\n",
    "            #all_notes_l_c.append(pm.Note(start=note.start + 8*beats*i, end=note.end + 8*beats*i, pitch=note.pitch, velocity=note.velocity))\n",
    "\n",
    "    generate_midi_with_melody_chord(fn, notes_list, all_notes_l_c, tempo = 60/beats)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_piece_to_result(pitch_files, pitch_files_chords, rhythm_files, rhythm_files_chords, fn, weight=1):\n",
    "    midi_data_list = [pm.PrettyMIDI(pitch_file) for pitch_file in pitch_files]\n",
    "    beats = [midi_data.get_beats()[1] for midi_data in midi_data_list]\n",
    "    quantized_notes_pitch_list = [get_quantized_notes_and_chords(pitch_files[i], pitch_files_chords[i])[0] for i in range(len(pitch_files))]\n",
    "    quantized_chords_pitch_list = [get_quantized_notes_and_chords(pitch_files[i], pitch_files_chords[i])[1] for i in range(len(pitch_files))]\n",
    "\n",
    "    quantized_notes_rhythm_list = [get_quantized_notes_and_chords(rhythm_files[i], rhythm_files_chords[i])[0] for i in range(len(rhythm_files))]\n",
    "    quantized_chords_rhythm_list = [get_quantized_notes_and_chords(rhythm_files[i], rhythm_files_chords[i])[1] for i in range(len(rhythm_files))]\n",
    "    \n",
    "    l_two_measures_pitch_1ist = [get_two_measures_list(quantized_notes_pitch_list[i], quantized_chords_pitch_list[i])[0] for i in range(len(quantized_notes_pitch_list))]\n",
    "    l_two_measures_chords_pitch_list = [get_two_measures_list(quantized_notes_pitch_list[i], quantized_chords_pitch_list[i])[1] for i in range(len(quantized_notes_pitch_list))]\n",
    "    \n",
    "    l_two_measures_rhythm_1ist = [get_two_measures_list(quantized_notes_rhythm_list[i], quantized_chords_rhythm_list[i])[0] for i in range(len(quantized_notes_rhythm_list))]\n",
    "    l_two_measures_chords_rhythm_list = [get_two_measures_list(quantized_notes_rhythm_list[i], quantized_chords_rhythm_list[i])[1] for i in range(len(quantized_notes_rhythm_list))]\n",
    "    \n",
    "    \n",
    "    \n",
    "    z_big_pitch_list = [get_z_list(l_two_measures_pitch_1ist[i], l_two_measures_chords_pitch_list[i])[0] for i in range(len(l_two_measures_pitch_1ist))]\n",
    "            \n",
    "    k = weight\n",
    "    if len(z_big_pitch_list) > 1:\n",
    "        for i in range(len(z_big_pitch_list[0])):\n",
    "            z_big_pitch_list[0][i][0] = z_big_pitch_list[0][i][0]*k\n",
    "            z_big_pitch_list[0][i][1] = z_big_pitch_list[0][i][1]*k\n",
    "    \n",
    "            for j in range(1, len(z_big_pitch_list)):\n",
    "                if i < len(z_big_pitch_list[j]):\n",
    "                    z_big_pitch_list[0][i][0] += z_big_pitch_list[j][i][0]*(1-k)/(len(z_big_pitch_list)-1)\n",
    "                    z_big_pitch_list[0][i][1] += z_big_pitch_list[j][i][1]*(1-k)/(len(z_big_pitch_list)-1)\n",
    "                    \n",
    "                else:\n",
    "                    break\n",
    "                    \n",
    "    \n",
    "                                                    \n",
    "    l_c = get_z_list(l_two_measures_pitch_1ist[0], l_two_measures_chords_pitch_list[0])[1]\n",
    "                                                              \n",
    "    z_big_rhythm_list = [get_z_list(l_two_measures_rhythm_1ist[i], l_two_measures_chords_rhythm_list[i])[0] for i in range(len(l_two_measures_rhythm_1ist))]\n",
    "    \n",
    "    if len(z_big_rhythm_list) > 1:\n",
    "        for i in range(len(z_big_rhythm_list[0])):\n",
    "            z_big_rhythm_list[0][i][0] = z_big_rhythm_list[0][i][0]*k\n",
    "            z_big_rhythm_list[0][i][1] = z_big_rhythm_list[0][i][1]*k\n",
    "    \n",
    "            for j in range(1, len(z_big_rhythm_list)):\n",
    "                if i < len(z_big_rhythm_list[j]):\n",
    "                    z_big_rhythm_list[0][i][0] += z_big_rhythm_list[j][i][0]*(1-k)/(len(z_big_rhythm_list)-1)\n",
    "                    z_big_rhythm_list[0][i][1] += z_big_rhythm_list[j][i][1]*(1-k)/(len(z_big_rhythm_list)-1)\n",
    "                    \n",
    "                else:\n",
    "                    break  \n",
    "    \n",
    "    generate_raw_piece(z_big_pitch_list[0], z_big_rhythm_list[0], l_c, beats[0], fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formalize melody\n",
    "f4 = open('chords4.txt', 'r')\n",
    "chords4 = []\n",
    "content = f4.readlines()\n",
    "for i in content:\n",
    "    chords4.append(i.split())\n",
    "f4.close()\n",
    "sample_midi = 'POP909_004_004.mid'\n",
    "sample_chord = chords4\n",
    "\n",
    "def formalize_melody(raw_midi, chord1, sample_midi, chord2):\n",
    "    from_piece_to_result([raw_midi], [chord1], [sample_midi], [chord2], f'{raw_midi[:-4]}_formalized.mid')\n",
    "    return f'{raw_midi[:-4]}_formalized.mid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Midi-Level Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation of chords 1: Keep striking\n",
    "def keep_striking(midi_data, quantized_chord_notes, beats):\n",
    "    new_notes = []\n",
    "    for note in quantized_chord_notes:\n",
    "        for i in range(note['end_quantized']-note['start_quantized']):\n",
    "            new_notes.append(pm.Note(start=(note['start_quantized'] + i)*beats[1]/4, end=(note['start_quantized'] + i + 1)*beats[1]/4, pitch=note['pitch'], velocity=70 if i == 0 else 50))\n",
    "        \n",
    "    midi_data.instruments[1].notes = new_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation of melody 1: Canon\n",
    "def canon(midi_data, quantized_melody_notes, beats):\n",
    "    for note in quantized_melody_notes:\n",
    "        midi_data.instruments[0].notes.append(pm.Note(start=(note['start_quantized'] + 4)*beats[1]/4, end=(note['end_quantized'] + 4)*beats[1]/4, pitch=note['pitch'], velocity=70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformation of melody 2: Note Interpolation\n",
    "def notes_interpolation(midi_data, quantized_melody_notes, beats):\n",
    "    for i in range(len(quantized_melody_notes)-1):\n",
    "        distance = quantized_melody_notes[i + 1]['start_quantized'] - quantized_melody_notes[i]['start_quantized']\n",
    "        if distance > 5 and distance <= 20:\n",
    "            pitch1 = quantized_melody_notes[i]\n",
    "            pitch2 = quantized_melody_notes[i+1]\n",
    "            while distance > 2:\n",
    "                midi_data.instruments[0].notes.append(pm.Note(start=(pitch1['start_quantized'] + distance // 2)*beats[1]/4, end=(pitch1['end_quantized'] + distance // 2)*beats[1]/4, pitch=(pitch1['pitch'] + pitch2['pitch'])//2, velocity=90))\n",
    "                pitch1 = quantized_melody_notes[-1]\n",
    "                distance = distance // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Midi level Reconstruction\n",
    "def midi_composition(fn, mode = \"\"):\n",
    "    midi_data = pm.PrettyMIDI(fn)\n",
    "    beats = midi_data.get_beats()\n",
    "    beat_id = np.arange(len(beats))\n",
    "    subbeat_id = np.arange(len(beats) * 4) / 4.0\n",
    "    keypoints = np.interp(subbeat_id, beat_id, beats)\n",
    "    keypoint_boundaries = (keypoints[1:] + keypoints[:-1]) / 2\n",
    "    \n",
    "    #chord note quantization\n",
    "    quantized_chord_notes = [{\n",
    "    'start_quantized': quantize(keypoint_boundaries, note.start),\n",
    "    'end_quantized': quantize(keypoint_boundaries, note.end),\n",
    "    'pitch': note.pitch,\n",
    "    'velocity' : note.velocity,\n",
    "    } for note in midi_data.instruments[1].notes]\n",
    "    \n",
    "    #melody note quantization\n",
    "    quantized_melody_notes = [{\n",
    "    'start_quantized': quantize(keypoint_boundaries, note.start),\n",
    "    'end_quantized': quantize(keypoint_boundaries, note.end),\n",
    "    'pitch': note.pitch,\n",
    "    'velocity' : note.velocity,\n",
    "    } for note in midi_data.instruments[0].notes]\n",
    "    \n",
    "    mode_list = mode.split()\n",
    "    if 'K' in mode_list:\n",
    "        keep_striking(midi_data, quantized_chord_notes, beats)\n",
    "        \n",
    "    if 'C' in mode_list:\n",
    "        canon(midi_data, quantized_melody_notes, beats)\n",
    "        \n",
    "    if 'I' in mode_list:\n",
    "        notes_interpolation(midi_data, quantized_melody_notes, beats)\n",
    "        \n",
    "    midi_data.write(fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Wave-Level Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_freq(midi_fn):\n",
    "    midi_file = pm.PrettyMIDI(midi_fn)\n",
    "    all_notes = []\n",
    "    end_times = []\n",
    "    for track in midi_file.instruments:\n",
    "        track_notes = np.zeros((len(track.notes), 4)) #[start, end, f, velo]\n",
    "        for i in range(len(track.notes)):\n",
    "            track_notes[i][0] = track.notes[i].start\n",
    "            track_notes[i][1] = track.notes[i].end\n",
    "            track_notes[i][2] = librosa.midi_to_hz(track.notes[i].pitch)\n",
    "            track_notes[i][3] = track.notes[i].velocity\n",
    "        end_time = track_notes[i][1]\n",
    "        end_times.append(end_time)\n",
    "        \n",
    "        # Sort the track notes array by start time (for chord track)\n",
    "        track_notes = np.array(sorted(track_notes, key=itemgetter(0)))\n",
    "        \n",
    "        # Separate the three notes if the track contains the chord\n",
    "        if track_notes[0][0] == track_notes[1][0] and track_notes[1][0] == track_notes[2][0]: # Determine whether the track is chord by the start time of the first three notes\n",
    "            sub_track_1 = np.array([track_notes[i*3] for i in range(len(track_notes)//3)])\n",
    "            sub_track_2 = np.array([track_notes[i*3+1] for i in range(len(track_notes)//3)])\n",
    "            sub_track_3 = np.array([track_notes[i*3+2] for i in range(len(track_notes)//3)])\n",
    "            track_notes = [sub_track_1, sub_track_2, sub_track_3]\n",
    "        all_notes.append(track_notes)\n",
    "        beat_length = midi_file.get_beats()[1]\n",
    "    return all_notes, beat_length, end_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_wave(A, f, t_start, t_end, overhang_depth, fm_depth, fm_freq, sr=44100):\n",
    "    \"\"\"\n",
    "    Generate original sound wave. FM is implemented in the function, \n",
    "    AM is used to avoid click\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : TYPE Integer\n",
    "        DESCRIPTION. The amplitude of the wave\n",
    "    f : TYPE Float\n",
    "        DESCRIPTION. The frequency of the wave\n",
    "    t : TYPE Float\n",
    "        DESCRIPTION. The duration of the sound\n",
    "    overhang_depth : TYPE Float\n",
    "        DESCRIPTION. The FM systhesis depth for overhang generation\n",
    "    fm_depth : TYPE Float\n",
    "        DESCRIPTION. The FM synthesis depth for vibrato\n",
    "    fm_freq : TYPE Float\n",
    "        DESCRIPTION. The FM synthesis frequency for vibrato\n",
    "    sr : TYPE Integer, optional\n",
    "        DESCRIPTION. The sample rate. The default is 44100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        DESCRIPTION. The original wave (only with FM and AM)\n",
    "\n",
    "    \"\"\"\n",
    "    a = [A * cos(2*pi*f*x + overhang_depth*cos(2*pi*f*x) + fm_depth*cos(2*pi*fm_freq*x))\\\n",
    "     for x in arange(0,t_end-t_start,1./sr)]\n",
    "    window = np.hanning(len(a))\n",
    "    #window[:1000] = [exp(x)-e+1 for x in arange(0, 1, 1/1000)]\n",
    "    #window[-1000:] = [exp(-1*x) for x in arange(0, 1, 1/1000)]\n",
    "    result = np.array(a)*window\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piano_sound(A, f, t_start, t_end, sr=44100):\n",
    "    Y = np.array([sin(2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) for x in arange(0,t_end-t_start,1./sr)])\n",
    "    Y += [sin(2 * 2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) / 2 for x in arange(0,t_end-t_start,1./sr)]\n",
    "    Y += [sin(3 * 2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) / 4 for x in arange(0,t_end-t_start,1./sr)]\n",
    "    Y += [sin(4 * 2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) / 8 for x in arange(0,t_end-t_start,1./sr)]\n",
    "    Y += [sin(5 * 2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) / 16 for x in arange(0,t_end-t_start,1./sr)]\n",
    "    Y += [sin(6 * 2 * pi * f * x) * exp(-0.0004 * 2 * pi * f * x) / 32 for x in arange(0,t_end-t_start,1./sr)]\n",
    "    window = np.hanning(len(Y))\n",
    "    #window[:1000] = np.array([exp(x)-e+1 for x in arange(0, 1, 1/1000)])\n",
    "    #window[-1000:] = np.array([exp(-x) for x in arange(0, 1, 1/1000)])\n",
    "    result = Y*window\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_piece(note_list, end_time, sr = 44100):\n",
    "    all_notes_wav = np.zeros((int(end_time*sr)))\n",
    "    for track in note_list:\n",
    "        if type(track)==list:\n",
    "            for sub_track in track:\n",
    "                for note in sub_track:\n",
    "                    note_wav = piano_sound(note[-1], note[2], note[0], note[1], sr)\n",
    "                    try:\n",
    "                        all_notes_wav[int(note[0]*sr):int(note[1]*sr)] += note_wav\n",
    "                    except:\n",
    "                        all_notes_wav[int(note[0]*sr):int(note[1]*sr)] += note_wav[:-1]\n",
    "        else:\n",
    "            for note in track:\n",
    "                note_wav = piano_sound(note[-1], note[2], note[0], note[1], sr)\n",
    "                try:\n",
    "                    all_notes_wav[int(note[0]*sr):int(note[1]*sr)] += note_wav\n",
    "                except:\n",
    "                    all_notes_wav[int(note[0]*sr):int(note[1]*sr)] += note_wav[:-1]\n",
    "    #sf.write(wav_fn, all_notes_wav, samplerate=sr)\n",
    "    return all_notes_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_chords(chord_note_list, unit_time):\n",
    "    separate_note_list = []\n",
    "    b = 0\n",
    "    for i in range(len(chord_note_list[0])):\n",
    "        note_number = round((chord_note_list[0][i][1]-chord_note_list[0][i][0]), 5)//round(unit_time, 5)\n",
    "        #print(note_number)\n",
    "        a = 0\n",
    "        while a < note_number:\n",
    "            ind = random.randint(0, 2)\n",
    "            if chord_note_list[ind][i][2] != b:\n",
    "                b = chord_note_list[ind][i][2]\n",
    "                separate_note_list.append(np.array([round(chord_note_list[0][i][0]+a*unit_time, 4), round(chord_note_list[0][i][0]+(a+1)*unit_time, 4),\\\n",
    "                                                   chord_note_list[ind][i][2], chord_note_list[ind][i][-1]]))\n",
    "                a += 1\n",
    "        try:\n",
    "            separate_note_list.append(np.array([round(chord_note_list[0][i][0]+(a)*unit_time, 4), round(chord_note_list[0][i+1][0], 4),\\\n",
    "                                                chord_note_list[0][i][2], chord_note_list[0][i][-1]]))\n",
    "        except:\n",
    "            pass\n",
    "    return np.array(separate_note_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_to_wav(midi_fn, chord_separation=False, sr = 44100):\n",
    "    all_notes, beat_length, end_times = midi_to_freq(midi_fn)\n",
    "    if chord_separation == False:\n",
    "        notes_for_chord = [all_notes[1]]\n",
    "    else:\n",
    "        separated_chord = separate_chords(all_notes[1], beat_length/4)\n",
    "        notes_for_chord = [separated_chord]\n",
    "    \n",
    "    melody = generate_piece([all_notes[0]], end_times[0], sr)\n",
    "    chord = generate_piece(notes_for_chord, end_times[1], sr)\n",
    "    \n",
    "    if len(melody) != len(chord):\n",
    "        melody_new = np.zeros(max(len(melody), len(chord)))\n",
    "        chord_new = np.zeros(max(len(melody), len(chord)))\n",
    "        melody_new[:len(melody)] += melody\n",
    "        chord_new[:len(chord)] += chord\n",
    "        melody = melody_new\n",
    "        chord = chord_new\n",
    "        del melody_new, chord_new\n",
    "        \n",
    "    return melody, chord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization.py:534: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models/2stems/model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.8/tkinter/__init__.py\", line 1883, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-27-00ce765c4adf>\", line 79, in generate\n",
      "    temp = melody_and_chords_extraction(fn)\n",
      "  File \"<ipython-input-6-dcd6645e53ed>\", line 2, in melody_and_chords_extraction\n",
      "    spleeter_separate(fn)\n",
      "  File \"<ipython-input-2-2062254cce4f>\", line 6, in spleeter_separate\n",
      "    os.mkdir(f'{file_name[:-4]}_processed_audio')\n",
      "FileExistsError: [Errno 17] File exists: 'demo1_processed_audio'\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "from tkinter.filedialog import askopenfilename\n",
    "\n",
    "class MusicStyleSynthesizer:\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        root.title(\"Music Style Synthesizer\")\n",
    "        root.geometry('1068x381+10+10')\n",
    "        mainframe = ttk.Frame(root, padding=\"3 3 12 12\")\n",
    "        mainframe.grid(column=0, row=0, sticky=(N, W, E, S))\n",
    "        root.columnconfigure(0, weight=1)\n",
    "        root.rowconfigure(0, weight=1)\n",
    "       \n",
    "        self.feet = StringVar()\n",
    "        feet_entry = ttk.Entry(mainframe, width=7, textvariable=self.feet)\n",
    "        feet_entry.grid(column=2, row=1, sticky=(W, E))\n",
    "        \n",
    "        self.feet2 = StringVar()\n",
    "        feet_entry = ttk.Entry(mainframe, width=7, textvariable=self.feet2)\n",
    "        feet_entry.grid(column=4, row=1, sticky=(W, E))\n",
    "        \n",
    "        self.feet3 = StringVar()\n",
    "        feet_entry = ttk.Entry(mainframe, width=7, textvariable=self.feet3)\n",
    "        feet_entry.grid(column=6, row=1, sticky=(W, E))\n",
    "        \n",
    "        self.feet4 = StringVar()\n",
    "        feet_entry = ttk.Entry(mainframe, width=7, textvariable=self.feet4)\n",
    "        feet_entry.grid(column=2, row=2, sticky=(W, E))\n",
    "        \n",
    "        self.feet5 = StringVar()\n",
    "        feet_entry = ttk.Entry(mainframe, width=7, textvariable=self.feet5)\n",
    "        feet_entry.grid(column=2, row=3, sticky=(W, E))\n",
    "        \n",
    "        self.meters = StringVar()\n",
    "\n",
    "        ttk.Label(mainframe, textvariable=self.meters).grid(column=4, row=3, sticky=(W, E))\n",
    "        ttk.Button(mainframe, text=\"Generate\", command=self.generate).grid(column=2, row=4, sticky=W)\n",
    "\n",
    "        ttk.Label(mainframe, text=\"Pitch Input Files\").grid(column=1, row=1, sticky=W)\n",
    "        ttk.Label(mainframe, text=\"Rhythm Input Files\").grid(column=3, row=1, sticky=W)\n",
    "        ttk.Label(mainframe, text=\"Weight for Major File\").grid(column=5, row=1, sticky=W)\n",
    "        ttk.Label(mainframe, text=\"Midi Composition Mode\").grid(column=1, row=2, sticky=E)\n",
    "        ttk.Label(mainframe, text=\"K:Keep Striking, C:Canon, I:Pitch Interpolation\").grid(column=4, row=2, sticky=W)\n",
    "        ttk.Label(mainframe, text=\"Chords Decomposition\").grid(column=1, row=3, sticky=W)\n",
    "        ttk.Label(mainframe, text=\"Enter: Yes/No\").grid(column=4, row=3, sticky=W)\n",
    "        #ttk.Label(mainframe, text=\"For filenames, please enter space before you enter the next filename\").grid(column=1, row=7, sticky=W)\n",
    "        #ttk.Label(mainframe, text=\"Major file is defined as the first file entered, the weight k will be assigned to this file, and the remaining weight will be distributed equally among other files\").grid(column=1, row=7, sticky=W)\n",
    "        \n",
    "        for child in mainframe.winfo_children(): \n",
    "            child.grid_configure(padx=5, pady=5)\n",
    "\n",
    "        feet_entry.focus()\n",
    "        root.bind(\"<Return>\", self.generate)\n",
    "        \n",
    "    def generate(self, *args):\n",
    "        try:\n",
    "            value1 = str(self.feet.get())\n",
    "            value2 = str(self.feet2.get())\n",
    "            k = float(self.feet3.get())\n",
    "            if k < 0 or k > 1:\n",
    "                self.meters.set(\"Invalid Weight\")\n",
    "                return\n",
    "                \n",
    "            mode = str(self.feet4.get())\n",
    "            chord_decomp = str(self.feet5.get())\n",
    "            \n",
    "            self.meters.set(\"Begin Processing\")\n",
    "            \n",
    "            list1 = [filename + '.wav' for filename in value1.split()]\n",
    "            list2 = [filename + '.wav' for filename in value2.split()]\n",
    "            \n",
    "            midi_input_l1 = []\n",
    "            midi_input_l2 = []\n",
    "            chords1 = []\n",
    "            chords2 = []\n",
    "            for fn in list1:\n",
    "                temp = melody_and_chords_extraction(fn)\n",
    "                midi_input_l1.append(temp[0])\n",
    "                chords1.append(temp[1])\n",
    "                \n",
    "            for fn in list2:\n",
    "                temp = melody_and_chords_extraction(fn)\n",
    "                midi_input_l2.append(temp[0])\n",
    "                chords2.append(temp[1]) \n",
    "             \n",
    "            formalized_input_l1 = []\n",
    "            formalized_input_l2 = []\n",
    "            for i in range(len(midi_input_l1)):\n",
    "                formalized_input_l1.append(formalize_melody(midi_input_l1[i], chords1[i], sample_midi, sample_chord))\n",
    "            \n",
    "            for j in range(len(midi_input_l2)):\n",
    "                formalized_input_l2.append(formalize_melody(sample_midi, sample_chord, midi_input_l2[j], chords2[j]))\n",
    "                \n",
    "            self.meters.set(\"Generating Piece\")\n",
    "            from_piece_to_result(formalized_input_l1, chords1, formalized_input_l2, chords2, \"Result.mid\", k)\n",
    "            \n",
    "            self.meters.set(\"Midi-Level Composing\")\n",
    "            midi_composition(\"Result.mid\", mode)\n",
    "            \n",
    "            self.meters.set(\"Synthesis\")\n",
    "            if chord_decomp == \"Yes\":\n",
    "                melody, chord = midi_to_wav(\"Result.mid\", chord_separation=True, sr = 44100)\n",
    "            else:\n",
    "                melody, chord = midi_to_wav(\"Result.mid\", chord_separation=False, sr = 44100)\n",
    "                    \n",
    "            data = melody + chord\n",
    "            sf.write(\"Result.wav\", data, 44100)\n",
    "            self.meters.set(\"New File Generated Successfully!!\")\n",
    "            \n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "root = Tk()\n",
    "MusicStyleSynthesizer(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
